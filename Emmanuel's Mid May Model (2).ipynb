{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Requirement already satisfied: imblearn==0.0 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from -r requirements.txt (line 1)) (0.0)You are using pip version 19.0.3, however version 20.1.1 is available.\nYou should consider upgrading via the 'python -m pip install --upgrade pip' command.\n\nRequirement already satisfied: nltk==3.5 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from -r requirements.txt (line 2)) (3.5)\nRequirement already satisfied: numpy==1.18.4 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from -r requirements.txt (line 3)) (1.18.4)\nRequirement already satisfied: pandas==1.0.3 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from -r requirements.txt (line 4)) (1.0.3)\nRequirement already satisfied: scikit_learn==0.23.1 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from -r requirements.txt (line 5)) (0.23.1)\nRequirement already satisfied: spacy==2.2.4 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from -r requirements.txt (line 6)) (2.2.4)\nRequirement already satisfied: xgboost==1.1.0 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from -r requirements.txt (line 7)) (1.1.0)\nRequirement already satisfied: xlrd==1.2.0 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from -r requirements.txt (line 8)) (1.2.0)\nRequirement already satisfied: easygui==0.98.1 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from -r requirements.txt (line 9)) (0.98.1)\nCollecting openpyxl==2.6.4 (from -r requirements.txt (line 10))\n  Using cached https://files.pythonhosted.org/packages/d6/26/eb28e975b7a37aad38d7ec4f7a0f652bdee6ecf36e6bd06f473c5af9b87b/openpyxl-2.6.4.tar.gz\nRequirement already satisfied: imbalanced-learn in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from imblearn==0.0->-r requirements.txt (line 1)) (0.6.2)\nRequirement already satisfied: click in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nltk==3.5->-r requirements.txt (line 2)) (7.1.2)\nRequirement already satisfied: joblib in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nltk==3.5->-r requirements.txt (line 2)) (0.15.1)\nRequirement already satisfied: regex in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nltk==3.5->-r requirements.txt (line 2)) (2020.5.14)\nRequirement already satisfied: tqdm in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nltk==3.5->-r requirements.txt (line 2)) (4.46.0)\nRequirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\micks\\appdata\\roaming\\python\\python37\\site-packages (from pandas==1.0.3->-r requirements.txt (line 4)) (2.8.1)\nRequirement already satisfied: pytz>=2017.2 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas==1.0.3->-r requirements.txt (line 4)) (2020.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit_learn==0.23.1->-r requirements.txt (line 5)) (2.0.0)\nRequirement already satisfied: scipy>=0.19.1 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit_learn==0.23.1->-r requirements.txt (line 5)) (1.4.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy==2.2.4->-r requirements.txt (line 6)) (2.23.0)\nRequirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy==2.2.4->-r requirements.txt (line 6)) (0.4.1)\nRequirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy==2.2.4->-r requirements.txt (line 6)) (1.0.0)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy==2.2.4->-r requirements.txt (line 6)) (2.0.3)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy==2.2.4->-r requirements.txt (line 6)) (3.0.2)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy==2.2.4->-r requirements.txt (line 6)) (1.0.2)\nRequirement already satisfied: setuptools in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy==2.2.4->-r requirements.txt (line 6)) (40.8.0)\nRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy==2.2.4->-r requirements.txt (line 6)) (0.6.0)\nRequirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy==2.2.4->-r requirements.txt (line 6)) (1.1.3)\nRequirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy==2.2.4->-r requirements.txt (line 6)) (1.0.2)\nRequirement already satisfied: thinc==7.4.0 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy==2.2.4->-r requirements.txt (line 6)) (7.4.0)\nCollecting jdcal (from openpyxl==2.6.4->-r requirements.txt (line 10))\n  Using cached https://files.pythonhosted.org/packages/f0/da/572cbc0bc582390480bbd7c4e93d14dc46079778ed915b505dc494b37c57/jdcal-1.4.1-py2.py3-none-any.whl\nCollecting et_xmlfile (from openpyxl==2.6.4->-r requirements.txt (line 10))\n  Using cached https://files.pythonhosted.org/packages/22/28/a99c42aea746e18382ad9fb36f64c1c1f04216f41797f2f0fa567da11388/et_xmlfile-1.0.1.tar.gz\nRequirement already satisfied: six>=1.5 in c:\\users\\micks\\appdata\\roaming\\python\\python37\\site-packages (from python-dateutil>=2.6.1->pandas==1.0.3->-r requirements.txt (line 4)) (1.14.0)\nRequirement already satisfied: idna<3,>=2.5 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4->-r requirements.txt (line 6)) (2.9)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4->-r requirements.txt (line 6)) (1.25.9)\nRequirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4->-r requirements.txt (line 6)) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4->-r requirements.txt (line 6)) (2020.4.5.1)\nRequirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\users\\micks\\appdata\\roaming\\python\\python37\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.4->-r requirements.txt (line 6)) (1.6.0)\nRequirement already satisfied: zipp>=0.5 in c:\\users\\micks\\appdata\\roaming\\python\\python37\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy==2.2.4->-r requirements.txt (line 6)) (3.1.0)\nInstalling collected packages: jdcal, et-xmlfile, openpyxl\n  Running setup.py install for et-xmlfile: started\n    Running setup.py install for et-xmlfile: finished with status 'done'\n  Running setup.py install for openpyxl: started\n    Running setup.py install for openpyxl: finished with status 'done'\nSuccessfully installed et-xmlfile-1.0.1 jdcal-1.4.1 openpyxl-2.6.4\n"
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Collecting nl_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/nl_core_news_sm-2.2.5/nl_core_news_sm-2.2.5.tar.gz#egg=nl_core_news_sm==2.2.5You are using pip version 19.0.3, however version 20.1.1 is available.\nYou should consider upgrading via the 'python -m pip install --upgrade pip' command.\n\n  Downloading https://github.com/explosion/spacy-models/releases/download/nl_core_news_sm-2.2.5/nl_core_news_sm-2.2.5.tar.gz (15.4MB)\nRequirement already satisfied: spacy>=2.2.2 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nl_core_news_sm==2.2.5) (2.2.4)\nRequirement already satisfied: numpy>=1.15.0 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.2.2->nl_core_news_sm==2.2.5) (1.18.4)\nRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.2.2->nl_core_news_sm==2.2.5) (0.6.0)\nRequirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.2.2->nl_core_news_sm==2.2.5) (0.4.1)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.2.2->nl_core_news_sm==2.2.5) (3.0.2)\nRequirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.2.2->nl_core_news_sm==2.2.5) (1.1.3)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.2.2->nl_core_news_sm==2.2.5) (2.0.3)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.2.2->nl_core_news_sm==2.2.5) (4.46.0)\nRequirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.2.2->nl_core_news_sm==2.2.5) (1.0.2)\nRequirement already satisfied: thinc==7.4.0 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.2.2->nl_core_news_sm==2.2.5) (7.4.0)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.2.2->nl_core_news_sm==2.2.5) (2.23.0)\nRequirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.2.2->nl_core_news_sm==2.2.5) (1.0.0)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.2.2->nl_core_news_sm==2.2.5) (1.0.2)\nRequirement already satisfied: setuptools in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.2.2->nl_core_news_sm==2.2.5) (40.8.0)\nRequirement already satisfied: certifi>=2017.4.17 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->nl_core_news_sm==2.2.5) (2020.4.5.1)\nRequirement already satisfied: idna<3,>=2.5 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->nl_core_news_sm==2.2.5) (2.9)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->nl_core_news_sm==2.2.5) (1.25.9)\nRequirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\micks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->nl_core_news_sm==2.2.5) (3.0.4)\nRequirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\users\\micks\\appdata\\roaming\\python\\python37\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->nl_core_news_sm==2.2.5) (1.6.0)\nRequirement already satisfied: zipp>=0.5 in c:\\users\\micks\\appdata\\roaming\\python\\python37\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->nl_core_news_sm==2.2.5) (3.1.0)\nInstalling collected packages: nl-core-news-sm\n  Running setup.py install for nl-core-news-sm: started\n    Running setup.py install for nl-core-news-sm: finished with status 'done'\nSuccessfully installed nl-core-news-sm-2.2.5\n✔ Download and installation successful\nYou can now load the model via spacy.load('nl_core_news_sm')\n"
    }
   ],
   "source": [
    "!{sys.executable} -m spacy download nl_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nl_core_news_sm\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import re\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "#from sklearn.metrics import roc_auc_score\n",
    "import xgboost as xgb\n",
    "import xlrd\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import openpyxl\n",
    "import joblib\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data \n",
    "data_1= pd.read_excel(\"./Heijmans data final.xlsx\")\n",
    "#data_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(18516, 6)\nColumn Count\nID_num                  18516\nIncident_Description    17909\nSub_Categories           3276\nReason_Categories        2340\nMain_Categories          3307\nIncident_Date           18516\ndtype: int64\n\n\nNull Count \nID_num                      0\nIncident_Description      607\nSub_Categories          15240\nReason_Categories       16176\nMain_Categories         15209\nIncident_Date               0\ndtype: int64\n\n\nUnique_main_categories: 10\nUnique_subcategories: 43\nUnique_reasons: 154\n\n\nMain_Categories\nAanrijding                                      5.624433\nAgressie en extreme lichamelijke inspanning     2.056244\nAndere vormen van contact met voorwerpen       14.454188\nBHV inzet op locatie                            3.386755\nGevaarlijke atmosfeer                           0.302389\nGevaarlijke stoffen, brand en explosies         3.144844\nGezondheidsschade                               0.514061\nMachines en handgereedschap                    12.760810\nVallen (van personen of voorwerpen)            45.539764\noverig                                         12.216510\ndtype: float64\nCount of new data frame \nID_num                  3307\nIncident_Description    3211\nSub_Categories          3276\nReason_Categories       2340\nMain_Categories         3307\nIncident_Date           3307\ndtype: int64\n\n\nDouble Checking for NaN\nID_num                    0\nIncident_Description     96\nSub_Categories           31\nReason_Categories       967\nMain_Categories           0\nIncident_Date             0\ndtype: int64\n"
    }
   ],
   "source": [
    "#re-naming columns\n",
    "data_1= data_1.rename(columns ={'Nr.': 'ID_num', \"Beschrijving incident\": \"Incident_Description\", \"Subcategorie ongeval\": \"Sub_Categories\", \"Categorie ongeval\": \"Main_Categories\", \"Subcategorie(2) ongeval\": \"Reason_Categories\", \n",
    "                                \"Datum incident\": \"Incident_Date\" })\n",
    "\n",
    "\n",
    "#selecting the columns to work with\n",
    "ID_num = data_1[\"ID_num\"]\n",
    "Incident_Description = data_1[\"Incident_Description\"]\n",
    "Sub_Categories = data_1[\"Sub_Categories\"]\n",
    "Reason_Categories= data_1[\"Reason_Categories\"]\n",
    "Main_Categories= data_1[\"Main_Categories\"]\n",
    "Incident_Date= data_1[\"Incident_Date\" ]\n",
    "\n",
    "#creating a new dataframe\n",
    "c1= pd.Series(ID_num, name = \"ID_num\")\n",
    "c2= pd.Series(Incident_Description, name = \"Incident_Description\")\n",
    "c3= pd.Series(Sub_Categories, name= \"Sub_Categories\")\n",
    "c4= pd.Series(Reason_Categories ,name= \"Reason_Categories\")\n",
    "c5= pd.Series(Main_Categories, name= \"Main_Categories\")\n",
    "c6= pd.Series(Incident_Date, name= \"Incident_Date\")\n",
    "\n",
    "df = pd.concat([c1,c2,c3,c4,c5,c6], axis=1)\n",
    "print(df.shape)\n",
    "\n",
    "# EDA- Exploratory Data Analysis\n",
    "\n",
    "#number of entries for each column\n",
    "column_count= df.count()\n",
    "print (\"Column Count\")\n",
    "print(column_count)\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "#number of null/NAN entries \n",
    "print(\"Null Count \") \n",
    "print(df.isnull().sum())\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "#Count of uniques entries\n",
    "a= df[\"Main_Categories\"].nunique()\n",
    "print(\"Unique_main_categories:\", a)\n",
    "c= df[\"Sub_Categories\"].nunique()\n",
    "print(\"Unique_subcategories:\", c)\n",
    "e= df[\"Reason_Categories\"].nunique()\n",
    "print(\"Unique_reasons:\", e)\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "# class distribution Percentages \n",
    "class_dist= (df.groupby(\"Main_Categories\").size()/df[\"Main_Categories\"].count())*100\n",
    "print(class_dist)\n",
    "\n",
    "# removing nan\n",
    "df[\"Main_Categories\"].replace(r'^\\s+$', np.nan, regex=True)\n",
    "mc_df_1=df.dropna(subset=[\"Main_Categories\"])\n",
    "\n",
    "print(\"Count of new data frame \")\n",
    "print(mc_df_1.count())\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"Double Checking for NaN\")\n",
    "print(mc_df_1.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing noise from the data \n",
    "mc_df_1=mc_df_1[Main_Categories!=\"overig\"]  # other category adds no value to data and reduces model's performance\n",
    "mc_df_2= mc_df_1[Main_Categories!=\"Gevaarlijke atmosfeer\"] # too few entries, the machine learns nothing\n",
    "mc_df= mc_df_2[Main_Categories!=\"Gezondheidsschade\"] # too few entries, the machine learns nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Main_Categories\nAanrijding                                      6.467316\nAgressie en extreme lichamelijke inspanning     2.364395\nAndere vormen van contact met voorwerpen       16.620306\nBHV inzet op locatie                            3.894298\nGevaarlijke stoffen, brand en explosies         3.616134\nMachines en handgereedschap                    14.673157\nVallen (van personen of voorwerpen)            52.364395\ndtype: float64\n"
    }
   ],
   "source": [
    "# class distribution Percentages to establish baseline\n",
    "class_dist= (mc_df.groupby(\"Main_Categories\").size()/mc_df[\"Main_Categories\"].count())*100\n",
    "print(class_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\micks\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# If stopwords load fails, load stopwords here\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = set(nltk.corpus.stopwords.words('dutch'))\n",
    "#add words that aren't in the NLTK stopwords list\n",
    "added_stopwords = [\"b\", \"u\", \"e\", \"ste\", \"kv\", \"aa\", \"a9\", \"mtr\", '+', '1e', '2e', '<', '>', 'a', 'a1', 'a12', 'a2', 'a9',\"één\", \"gis\", \"app\", \"h\", 'o.a.', 'oa', 'en/of', \"uur\", \"km\", \"mm\", \"v\", \"kg\", \"cm\", \"mm²\", \"mm2\", \"0ngever\", \"e\", \"de\", \"m\"]\n",
    "updated_stopword_list = stopword_list.union(added_stopwords)\n",
    "#print(updated_stopword_list)\n",
    "\n",
    "# Dutch spacy tokens \n",
    "nlp = nl_core_news_sm.load()\n",
    "\n",
    "def custom_lemmatizer(data):\n",
    "    return [(token.lemma_) for token in nlp(data) if not token.is_digit and not token.is_punct and not token.is_space]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'die', 'haar', 'voor', 'veel', 'km', 'kan', 'zo', 'iets', 'tot', 'om', 'ja', 'nu', 'over', 'van', 'in', 'mtr', 'is', 'men', '<', 'uw', 'dat', 'tegen', 'als', 'was', '1e', 'a12', 'één', 'der', '2e', 'gis', 'oa', 'onder', 'wat', 'uit', 'omdat', 'kv', 'o.a.', 'a9', 'op', 'zonder', 'ge', 'mm²', 'er', 'alles', 'deze', 'een', 'mm', 'naar', 'meer', 'je', 'waren', 'de', 'wil', 'hoe', 'door', 'hem', 'h', 'aan', 'nog', 'geweest', 'zou', 'dan', 'al', 'zelf', 'kon', '>', 'na', 'had', 'maar', 'heb', 'cm', 'ben', 'mm2', 'en/of', 'zal', '+', 'hebben', 'ik', 'a', 'wordt', 'a2', 'kg', 'wezen', 'want', 'm', 'het', 'mijn', 'hij', 'kunnen', 'me', 'doen', 'heeft', 'wie', 'ons', 'werd', 'reeds', 'niet', 'uur', 'a1', 'met', 'bij', 'zijn', 'iemand', 'ze', 'zich', 'geen', 'altijd', 'u', 'doch', 'andere', 'e', 'moet', 'en', 'niets', 'te', 'v', 'toch', 'mij', 'dus', 'ste', '0ngever', 'b', 'eens', 'dit', 'zij', 'daar', 'worden', 'hun', 'ook', 'toen', 'of', 'app', 'aa', 'hier'}\n"
    }
   ],
   "source": [
    "print(updated_stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "text = mc_df[\"Incident_Description\"].astype(str)\n",
    "label = mc_df[\"Main_Categories\"]\n",
    "x_text_train, text_test, label_train, label_test, = model_selection.train_test_split(text,label, test_size = 0.25)\n",
    "\n",
    "X_train, X_test, y_train, y_test \n",
    "    = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    " X_train, X_val, y_train, y_val \n",
    "    = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 𝑇𝐹×𝐼𝐷𝐹\n",
    "# word level tf-idf\n",
    "tfidf_vectorizer= TfidfVectorizer(max_features=12000,  stop_words=updated_stopword_list, max_df= 0.5, tokenizer=custom_lemmatizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\byri\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "C:\\Users\\byri\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['0ngev', 'moeten', 'sen', 'wijzen', 'willen', 'zullen'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "#fit & transform for cross validation \n",
    "train_tfidf = tfidf_vectorizer.fit(text)\n",
    "tfidf_text_v= train_tfidf.transform(text)\n",
    "\n",
    "# fit and transform for test  & train\n",
    "tfidf_text_v_train= train_tfidf.transform(text_train)\n",
    "tfidf_text_v_test= train_tfidf.transform(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score (test): 0.6648122392211405\n",
      "Accuracy score (training): 0.9281409364858599\n"
     ]
    }
   ],
   "source": [
    "#basic XGB model\n",
    "xgb_cl=xgb.XGBClassifier(random_state=4)\n",
    "xgb_cl.fit(tfidf_text_v_train, label_train)\n",
    "xgb_cl.score(tfidf_text_v_test, label_test)\n",
    "print(\"Accuracy score (test):\",xgb_cl.score(tfidf_text_v_test, label_test))\n",
    "print(\"Accuracy score (training):\", xgb_cl.score(tfidf_text_v_train, label_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score (test): 0.6717663421418637\n",
      "Accuracy score (training): 0.9304589707927677\n"
     ]
    }
   ],
   "source": [
    "#hyperparameter tuning- XGB model with early stopping and balanced weights and macros as metrics\n",
    "xgb_cl=xgb.XGBClassifier(random_state=4, class_weight=\"balanced\", metrics='f1', early_stopping_rounds=50)\n",
    "xgb_cl.fit(tfidf_text_v_train, label_train)\n",
    "xgb_cl.score(tfidf_text_v_test, label_test)\n",
    "print(\"Accuracy score (test):\",xgb_cl.score(tfidf_text_v_test, label_test))\n",
    "print(\"Accuracy score (training):\", xgb_cl.score(tfidf_text_v_train, label_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             precision    recall  f1-score   support\n",
      "\n",
      "                                 Aanrijding       0.76      0.50      0.60        44\n",
      "Agressie en extreme lichamelijke inspanning       0.00      0.00      0.00        14\n",
      "   Andere vormen van contact met voorwerpen       0.59      0.37      0.46       126\n",
      "                       BHV inzet op locatie       0.79      0.52      0.62        29\n",
      "    Gevaarlijke stoffen, brand en explosies       0.18      0.07      0.10        30\n",
      "                Machines en handgereedschap       0.56      0.52      0.54       102\n",
      "        Vallen (van personen of voorwerpen)       0.72      0.92      0.81       374\n",
      "\n",
      "                                   accuracy                           0.67       719\n",
      "                                  macro avg       0.51      0.41      0.45       719\n",
      "                               weighted avg       0.64      0.67      0.64       719\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels_predict_xgb= xgb_cl.predict(tfidf_text_v_test)\n",
    "print(classification_report(label_test, labels_predict_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 22   0   1   0   1   0  20]\n",
      " [  0   0   0   0   0   1  13]\n",
      " [  1   1  47   0   3  25  49]\n",
      " [  1   0   0  15   0   2  11]\n",
      " [  0   0   4   0   2   4  20]\n",
      " [  2   5  17   0   4  53  21]\n",
      " [  3   2  10   4   1  10 344]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(label_test, labels_predict_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\byri\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "C:\\Users\\byri\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['0ngev', 'moeten', 'sen', 'wijzen', 'willen', 'zullen'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "#Feature/ Noise reduction with min_df\n",
    "tfidf_vectorizer_2= TfidfVectorizer(max_features= 8000,  stop_words=updated_stopword_list, max_df= 0.5, min_df=0.05, tokenizer=custom_lemmatizer)\n",
    "#fit & transform for cross validation \n",
    "train_tfidf2 =tfidf_vectorizer_2.fit(text)\n",
    "tfidf_text_v2= train_tfidf2.transform(text)\n",
    "\n",
    "# fit and transform for test  & train\n",
    "tfidf_text_v_train2= train_tfidf2.transform(text_train)\n",
    "tfidf_text_v_test2= train_tfidf2.transform(text_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score (test): 0.521557719054242\n",
      "Accuracy score (training): 0.6226240148354195\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xgb_cl=xgb.XGBClassifier(random_state=4, class_weight=\"balanced\", metrics='mac', early_stopping_rounds=10)\n",
    "xgb_cl.fit(tfidf_text_v_train2, label_train)\n",
    "xgb_cl.score(tfidf_text_v_test2, label_test)\n",
    "print(\"Accuracy score (test):\",xgb_cl.score(tfidf_text_v_test2, label_test))\n",
    "print(\"Accuracy score (training):\", xgb_cl.score(tfidf_text_v_train2, label_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             precision    recall  f1-score   support\n",
      "\n",
      "                                 Aanrijding       0.00      0.00      0.00        44\n",
      "Agressie en extreme lichamelijke inspanning       0.00      0.00      0.00        14\n",
      "   Andere vormen van contact met voorwerpen       0.31      0.10      0.15       126\n",
      "                       BHV inzet op locatie       0.00      0.00      0.00        29\n",
      "    Gevaarlijke stoffen, brand en explosies       0.00      0.00      0.00        30\n",
      "                Machines en handgereedschap       0.41      0.22      0.28       102\n",
      "        Vallen (van personen of voorwerpen)       0.56      0.91      0.69       374\n",
      "\n",
      "                                   accuracy                           0.52       719\n",
      "                                  macro avg       0.18      0.17      0.16       719\n",
      "                               weighted avg       0.40      0.52      0.43       719\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\byri\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "labels_predict_xgb= xgb_cl.predict(tfidf_text_v_test2)\n",
    "print(classification_report(label_test, labels_predict_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   3  41]\n",
      " [  0   0   1   0   0   0  13]\n",
      " [  2   1  12   0   0  12  99]\n",
      " [  0   0   0   0   0   0  29]\n",
      " [  1   0   3   0   0   4  22]\n",
      " [  3   0  11   0   1  22  65]\n",
      " [  2   4  12   0   2  13 341]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(label_test, labels_predict_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter turning with Gridsearch\n",
    "model_params= {\"svm\": {\"model\":svm.SVC(gamma=\"auto\"), \"params\": {\"C\": [1,10,20], \"kernel\":[\"rbf\", \"linear\"], \"penalty\":[\"l1\", \"L2\"], \"dual\": [\"False\", \"True\"], \"loss\":[\"squared_hinge\"], \"class_weight\":[0.05, 0.95, 1, 20] }}, \n",
    "              \"decision_tree\": {\"model\":DecisionTreeClassifier(),\"params\": {\"n_estimators\": [1,10], \"criterion\":['gini','entropy'], \"class_weight\":[0.05, 0.95, 1, 20]}}, \n",
    "               \"naive_bayes\":{\"model\":MultinomialNB(),\"params\": {\"C\": [1,10],\"class_weight\":[0.05, 0.95, 1, 20] }},\n",
    "               \"kNN\":{\"model\":KNeighborsClassifier(),\"params\": { \"n_neighbors\":[3,5,10], \"weights\":[\"uniform\", \"distance\"], \"algorithm\":['ball_tree','kd_tree'],\"class_weight\":[0.05, 0.95, 1, 20] }},\n",
    "               \"xgb_cl\":{\"model\":xgb.XGBClassifier(random_state=4),\"params\": { \"n_estimators\": [50,100], \"learning_rate\":[0.1,0.5,1.0],\"class_weight\":[0.05, 0.95, 1, 20] }}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=[]\n",
    "\n",
    "for model_name, mp in model_params.items():\n",
    "    clf= GridSearchCV(mp[\"model\"], mp [\"params\"], cv=5, return_train_score=False )\n",
    "clf.fit(tfidf_text_v_train, label_train)\n",
    "scores.append({\"model\": model_name, \"best_score\":clf.best_score_, \"best_params\": clf.best_params_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gridsearch parameter turning sepcifically for XGB model\n",
    "model_params= {\"xgb_cl\":{\"model\":xgb.XGBClassifier(random_state=2),\"params\": { \"n_estimators\": [100,150], \n",
    "                                                                             \"learning_rate\":[0.5,1.0],\n",
    "                                                                             \"class_weight\":[0.05, 0.10, 0.15, 5],\n",
    "                                                                             \"max_depth\":[3,5,7],\"min_child_weight\":[1,1.5],\n",
    "                                                                             \"scale_pos_weight\":[0.5, 0.7, 1,1.5]}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=1)\n",
    "scores=[]\n",
    "\n",
    "for model_name, mp in model_params.items():\n",
    "    clf= GridSearchCV(mp[\"model\"], mp [\"params\"], cv=cv, return_train_score=False )\n",
    "clf.fit(tfidf_text_v_train, label_train)\n",
    "scores.append({\"model\": model_name, \"best_score\":clf.best_score_, \"best_params\": clf.best_params_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'model': 'xgb_cl', 'best_score': 0.6882315029646817, 'best_params': {'class_weight': 0.05, 'learning_rate': 0.5, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 100, 'scale_pos_weight': 0.5}}]\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score (test): 0.6787204450625869\n",
      "Accuracy score (training): 0.9077422345850719\n"
     ]
    }
   ],
   "source": [
    "#Getting the test & accuracy scores\n",
    "xgb_cl_op=xgb.XGBClassifier(random_state=4, learning_rate= 0.5, max_depth= 3,\n",
    "                            min_child_weight= 1, \n",
    "                            n_estimators= 100,\n",
    "                            class_weight=0.05, \n",
    "                            metrics='f1', \n",
    "                            early_stopping_rounds=50, \n",
    "                            scale_pos_weight=0.5)\n",
    "xgb_cl_op.fit(tfidf_text_v_train, label_train)\n",
    "xgb_cl_op.score(tfidf_text_v_test, label_test)\n",
    "print(\"Accuracy score (test):\",xgb_cl_op.score(tfidf_text_v_test, label_test))\n",
    "print(\"Accuracy score (training):\", xgb_cl_op.score(tfidf_text_v_train, label_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             precision    recall  f1-score   support\n",
      "\n",
      "                                 Aanrijding       0.82      0.52      0.64        44\n",
      "Agressie en extreme lichamelijke inspanning       0.08      0.07      0.08        14\n",
      "   Andere vormen van contact met voorwerpen       0.63      0.37      0.47       126\n",
      "                       BHV inzet op locatie       0.82      0.48      0.61        29\n",
      "    Gevaarlijke stoffen, brand en explosies       0.09      0.03      0.05        30\n",
      "                Machines en handgereedschap       0.55      0.54      0.54       102\n",
      "        Vallen (van personen of voorwerpen)       0.73      0.93      0.82       374\n",
      "\n",
      "                                   accuracy                           0.68       719\n",
      "                                  macro avg       0.53      0.42      0.46       719\n",
      "                               weighted avg       0.66      0.68      0.65       719\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels_predict_xgb= xgb_cl_op.predict(tfidf_text_v_test)\n",
    "print(classification_report(label_test, labels_predict_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 23   0   1   0   0   0  20]\n",
      " [  0   1   0   0   0   1  12]\n",
      " [  1   2  47   0   5  25  46]\n",
      " [  1   0   0  14   0   2  12]\n",
      " [  0   1   4   0   1   6  18]\n",
      " [  1   6  15   0   4  55  21]\n",
      " [  2   2   8   3   1  11 347]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(label_test, labels_predict_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\byri\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "C:\\Users\\byri\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['0ngev', 'moeten', 'sen', 'wijzen', 'willen', 'zullen'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "#Minimize features to reduce overfitting via max_def & min_def\n",
    "# increase number of stop words\n",
    "tfidf_vectorizer= TfidfVectorizer(max_features=14000,  stop_words=updated_stopword_list, max_df= 0.5, min_df=0.08, tokenizer=custom_lemmatizer)\n",
    "\n",
    "#fit & transform for cross validation \n",
    "train_tfidf = tfidf_vectorizer.fit(text)\n",
    "tfidf_text_v= train_tfidf.transform(text)\n",
    "\n",
    "# fit and transform for test  & train\n",
    "tfidf_text_v_train= train_tfidf.transform(text_train)\n",
    "tfidf_text_v_test= train_tfidf.transform(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score (test): 0.49235048678720444\n",
      "Accuracy score (training): 0.5605006954102921\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xgb_cl_op=xgb.XGBClassifier(random_state=4, learning_rate= 0.5, max_depth= 3,\n",
    "                            min_child_weight= 1, \n",
    "                            n_estimators= 100,\n",
    "                            class_weight=0.05, \n",
    "                            metrics='f1', \n",
    "                            early_stopping_rounds=50, \n",
    "                            scale_pos_weight=1)\n",
    "xgb_cl_op.fit(tfidf_text_v_train, label_train)\n",
    "xgb_cl_op.score(tfidf_text_v_test, label_test)\n",
    "print(\"Accuracy score (test):\",xgb_cl_op.score(tfidf_text_v_test, label_test))\n",
    "print(\"Accuracy score (training):\", xgb_cl_op.score(tfidf_text_v_train, label_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        46\n",
      "           1       0.00      0.00      0.00        20\n",
      "           2       0.33      0.01      0.01       139\n",
      "           3       0.00      0.00      0.00        24\n",
      "           4       0.00      0.00      0.00        30\n",
      "           5       0.38      0.21      0.27       115\n",
      "           6       0.50      0.95      0.66       345\n",
      "\n",
      "    accuracy                           0.49       719\n",
      "   macro avg       0.17      0.17      0.13       719\n",
      "weighted avg       0.37      0.49      0.36       719\n",
      "\n",
      "[[  0   0   0   0   0   3  43]\n",
      " [  0   0   0   0   0   2  18]\n",
      " [  0   0   1   0   0  17 121]\n",
      " [  0   0   0   0   0   0  24]\n",
      " [  0   0   0   0   0   4  26]\n",
      " [  0   0   0   0   0  24  91]\n",
      " [  0   0   2   0   0  14 329]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\byri\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "labels_predict_xgb= xgb_cl_op.predict(tfidf_text_v_test)\n",
    "print(classification_report(label_test, labels_predict_xgb))\n",
    "print(confusion_matrix(label_test, labels_predict_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\byri\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "C:\\Users\\byri\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['0ngev', 'moeten', 'sen', 'wijzen', 'willen', 'zullen'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "#Undersample the majority class\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler()\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "tfidf_vectorizer_2= TfidfVectorizer(max_features= 12000,  stop_words=updated_stopword_list, max_df= 0.5, tokenizer=custom_lemmatizer)\n",
    "#fit & transform for cross validation \n",
    "train_tfidf2 =tfidf_vectorizer_2.fit(text)\n",
    "tfidf_text_v2= train_tfidf2.transform(text)\n",
    "\n",
    "# fit and transform for test  & train\n",
    "tfidf_text_v_train2= train_tfidf2.transform(text_train)\n",
    "tfidf_text_v_test2= train_tfidf2.transform(text_test)\n",
    "\n",
    "fs = SelectKBest(chi2, k=\"all\")\n",
    "\n",
    "fs.fit(tfidf_text_v_train, text_train)\n",
    "\n",
    "X_train_fs = fs.transform(tfidf_text_v_train)\n",
    "\n",
    "X_test_fs = fs.transform(tfidf_text_v_test)\n",
    "\n",
    "\n",
    "text_train_rus, label_train_rus = rus.fit_resample(X_train_fs, label_train)\n",
    "\n",
    "text_test_rus, label_test_rus = rus.fit_resample(X_test_fs, label_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15577190542420027"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_cl=xgb.XGBClassifier(random_state=4, learning_rate= 0.5, max_depth= 3, min_child_weight= 1, n_estimators= 100, scale_pos_weight=1, class_weight=0.05, metrics='mac', early_stopping_rounds=50)\n",
    "xgb_cl.fit(text_train_rus, label_train_rus)\n",
    "xgb_cl.score(X_test_fs , label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USING PAST YEARS TO PREDICTION FUTURE YEARS' CATEGORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train Dataset: (984, 5)\nTest Dataset: (1892, 5)\n"
    }
   ],
   "source": [
    "#Divide dataset 90:10 \n",
    "mc_df[\"Incident_Date\"] = pd.to_datetime(mc_df[\"Incident_Date\"], format='%Y%m%d')\n",
    "mc_df = mc_df.set_index(\"Incident_Date\")\n",
    "mc_df = mc_df.sort_index()\n",
    "df_ey = mc_df['2006-09-11':'2017-04-10']\n",
    "df_ly  = mc_df['2017-04-11':]\n",
    "print('Train Dataset:',df_ey.shape)\n",
    "print('Test Dataset:',df_ly.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = df_ey[\"Incident_Description\"].astype(str)\n",
    "text_1=text_1.to_numpy()\n",
    "label_1 = df_ey[\"Main_Categories\"]\n",
    "text_1_train, text_1_test, label_1_train, label_1_test, = model_selection.train_test_split(text_1,label_1, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pickle\n",
    "train_tfidf = tfidf_vectorizer.fit(text_1)\n",
    "tfidf_text_v= train_tfidf.transform(text_1)\n",
    "# joblib.dump(train_tfidf, \"tfidf_vectorizer.sav\")\n",
    "pickle.dump(train_tfidf, open('tfidf_vectorizer.pickle', 'wb'))\n",
    "# fit and transform for test  & train\n",
    "tfidf_text_v_train= train_tfidf.transform(text_1_train)\n",
    "tfidf_text_v_test= train_tfidf.transform(text_1_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "SVM_P score (test): 0.8373983739837398\nSVM_P  score (training): 0.5989159891598916\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Category Ongeval(pototype).sav']"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "\n",
    "model_name = \"Category Ongeval(pototype).sav\"\n",
    "svm = LinearSVC(penalty=\"l1\", loss='squared_hinge',dual=False )\n",
    "SVM_cl_p = CalibratedClassifierCV(svm)\n",
    "SVM_cl_p.fit(tfidf_text_v_train, label_1_train)\n",
    "SVM_cl_p.fit(tfidf_text_v_test, label_1_test)\n",
    "print(\"SVM_P score (test):\",SVM_cl_p.score(tfidf_text_v_test, label_1_test))\n",
    "print(\"SVM_P  score (training):\", SVM_cl_p.score(tfidf_text_v_train, label_1_train))\n",
    "\n",
    "joblib.dump(SVM_cl_p, model_name)\n",
    "#print (\"SVM_P Score:\", SVM_cl_p.score(tfidf_text_v_test, label_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Test, Validation split with cross validation on test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting up the data\n",
    "\n",
    "Split:\n",
    "- CV Train (80%):\n",
    "    - Train (50%):\n",
    "        - Train_1 (50%)\n",
    "        - Train_2 (50%)\n",
    "    - Train (50%):\n",
    "        - Train_3 (50%)\n",
    "        - Train_4 (50%)\n",
    "- Test (20%)\n",
    "\n",
    "##### Variables to be used :-\n",
    "- Test Data:\n",
    "    - x_test_set\n",
    "    - y_test_set\n",
    "- Train Data:\n",
    "    - x_train_1, y_train_1\n",
    "    - x_train_2, y_train_2\n",
    "    - x_train_3, y_train_3\n",
    "    - x_train_4, y_train_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_ey[\"Incident_Description\"].astype(str)\n",
    "feature_vectorizer = tfidf_vectorizer.fit(features)\n",
    "labels = df_ey[\"Main_Categories\"]\n",
    "# Splitting data into train and test data (80/20)\n",
    "X_train, x_test_set, Y_train, y_test_set, = model_selection.train_test_split(features, labels, stratify=labels, test_size = 0.2, random_state=3)\n",
    "\n",
    "# Splitting train data into train and validation data 4 ways (25/25/25/25)\n",
    "pre_x_train, pre_x_cv, pre_y_train, pre_y_cv, = model_selection.train_test_split(X_train, Y_train, stratify=Y_train, test_size = 0.5, random_state=3)\n",
    "\n",
    "x_train_1, x_train_2, y_train_1, y_train_2 = model_selection.train_test_split(pre_x_train, pre_y_train, stratify=pre_y_train, test_size = 0.5, random_state=3)\n",
    "x_train_3, x_train_4, y_train_3, y_train_4 = model_selection.train_test_split(pre_x_cv, pre_y_cv, stratify=pre_y_cv, test_size = 0.5, random_state=3)\n",
    "\n",
    "x_test_transformed = feature_vectorizer.transform(x_test_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just checking the shape of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Testing features dataset shape:  (197,)\nTesting labels dataset shape:  (197,)\nTraining features dataset 1 shape:  (196,)\nTraining labels dataset 1 shape:  (196,)\nTraining features dataset 2 shape:  (197,)\nTraining labels dataset 2 shape:  (197,)\nTraining features dataset 3 shape:  (197,)\nTraining labels dataset 3 shape:  (197,)\nTraining features dataset 4 shape:  (197,)\nTraining labels dataset 4 shape:  (197,)\n"
    }
   ],
   "source": [
    "print(\"Testing features dataset shape: \", x_test_set.shape)\n",
    "print(\"Testing labels dataset shape: \", y_test_set.shape)\n",
    "\n",
    "print(\"Training features dataset 1 shape: \", x_train_1.shape)\n",
    "print(\"Training labels dataset 1 shape: \", y_train_1.shape)\n",
    "print(\"Training features dataset 2 shape: \", x_train_2.shape)\n",
    "print(\"Training labels dataset 2 shape: \", y_train_2.shape)\n",
    "print(\"Training features dataset 3 shape: \", x_train_3.shape)\n",
    "print(\"Training labels dataset 3 shape: \", y_train_3.shape)\n",
    "print(\"Training features dataset 4 shape: \", x_train_4.shape)\n",
    "print(\"Training labels dataset 4 shape: \", y_train_4.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to train this model with some kind of cross validation method... hopefully "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Transformed Labels at Step-1\nTransformed Labels at Step-2\nTransformed Labels at Step-3\nTransformed Labels at Step-4\nSVM_P score (test): 0.5482233502538071\n"
    }
   ],
   "source": [
    "model_name = \"Category Ongeval(pototype 2).sav\"\n",
    "svm = LinearSVC(penalty=\"l1\", loss='squared_hinge',dual=False )\n",
    "SVM_model = CalibratedClassifierCV(svm)\n",
    "\n",
    "train_list = [\n",
    "    [x_train_1, y_train_1],\n",
    "    [x_train_2, y_train_2],\n",
    "    [x_train_3, y_train_3],\n",
    "    [x_train_4, y_train_4]\n",
    "]\n",
    "\n",
    "size = len(train_list)\n",
    "\n",
    "for index in range(0, size):\n",
    "    train_features = None\n",
    "    train_labels = None\n",
    "    for i in range(0, size):\n",
    "        if(index != i):\n",
    "            # transformed = feature_vectorizer.transform(train_list[i][0])\n",
    "            # print(\"Transformed Labels at Step-{} : {}\".format(index + 1, i + 1))\n",
    "            # SVM_model.fit(transformed, train_list[i][1])\n",
    "            if(type(train_labels) == type(train_list[index])):\n",
    "                train_features.append(train_list[i][0], ignore_index=True)\n",
    "                train_labels.append(train_list[i][1], ignore_index=True)\n",
    "            else:\n",
    "                train_features = train_list[i][0]\n",
    "                train_labels = train_list[i][1]\n",
    "\n",
    "    transformed_features = feature_vectorizer.transform(train_features)\n",
    "    # transformed_labels = feature_vectorizer.transform(train_labels)\n",
    "    print(\"Transformed Labels at Step-{}\".format(index + 1))\n",
    "\n",
    "    SVM_model.fit(transformed_features, train_labels)\n",
    "print(\"SVM_P score (test):\",SVM_model.score(x_test_transformed, y_test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Transformed Labels at Step-1\nTransformed Labels at Step-2\nTransformed Labels at Step-3\nTransformed Labels at Step-4\nXG Boost score (test): 0.5177664974619289\n"
    }
   ],
   "source": [
    "model_name = \"Category Ongeval(pototype 2).sav\"\n",
    "# svm = LinearSVC(penalty=\"l1\", loss='squared_hinge',dual=False )\n",
    "# SVM_model = CalibratedClassifierCV(svm)\n",
    "xgb_model=xgb.XGBClassifier(random_state=4, learning_rate= 0.5, max_depth= 3,\n",
    "                            min_child_weight= 1, \n",
    "                            n_estimators= 100)\n",
    "\n",
    "train_list = [\n",
    "    [x_train_1, y_train_1],\n",
    "    [x_train_2, y_train_2],\n",
    "    [x_train_3, y_train_3],\n",
    "    [x_train_4, y_train_4]\n",
    "]\n",
    "\n",
    "size = len(train_list)\n",
    "\n",
    "for index in range(0, size):\n",
    "    train_features = None\n",
    "    train_labels = None\n",
    "    for i in range(0, size):\n",
    "        if(index != i):\n",
    "            # transformed = feature_vectorizer.transform(train_list[i][0])\n",
    "            # print(\"Transformed Labels at Step-{} : {}\".format(index + 1, i + 1))\n",
    "            # SVM_model.fit(transformed, train_list[i][1])\n",
    "            if(type(train_labels) == type(train_list[index])):\n",
    "                train_features.append(train_list[i][0], ignore_index=True)\n",
    "                train_labels.append(train_list[i][1], ignore_index=True)\n",
    "            else:\n",
    "                train_features = train_list[i][0]\n",
    "                train_labels = train_list[i][1]\n",
    "\n",
    "    transformed_features = feature_vectorizer.transform(train_features)\n",
    "    # transformed_labels = feature_vectorizer.transform(train_labels)\n",
    "    print(\"Transformed Labels at Step-{}\".format(index + 1))\n",
    "\n",
    "    xgb_model.fit(transformed_features, train_labels)\n",
    "print(\"XG Boost score (test):\",xgb_model.score(x_test_transformed, y_test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0.59898477, 0.67005076, 0.6751269 , 0.6751269 , 0.62755102])"
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "svm = LinearSVC(penalty=\"l1\", loss='squared_hinge',dual=False )\n",
    "SVM_model_test = CalibratedClassifierCV(svm)\n",
    "transformed_features = feature_vectorizer.transform(features)\n",
    "cross_val_score(SVM_model_test, transformed_features, labels, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0.50761421, 0.61928934, 0.70558376, 0.67005076, 0.60204082])"
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "xgb_model_test=xgb.XGBClassifier(random_state=4, learning_rate= 0.5, max_depth= 3,\n",
    "                            min_child_weight= 1, \n",
    "                            n_estimators= 100)\n",
    "transformed_features = feature_vectorizer.transform(features)\n",
    "cross_val_score(xgb_model_test, transformed_features, labels, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'type'>\n"
    }
   ],
   "source": [
    "test = None\n",
    "print(type(type(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ============================================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_val = df_ly[\"Incident_Description\"].astype(str)\n",
    "text_val=text_val.to_numpy()\n",
    "label_val = df_ly[\"Main_Categories\"]\n",
    "#val_tfidf = tfidf_vectorizer.fit(text_val)\n",
    "tfidf_text_val= train_tfidf.transform(text_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'numpy.ndarray'>\n1892\n<class 'numpy.ndarray'>\n13244\n"
    }
   ],
   "source": [
    "classes = list(np.unique(df_ly.Main_Categories))\n",
    "prob = SVM_cl_p.predict(tfidf_text_val) #Get predictions\n",
    "prob_score = SVM_cl_p.predict_proba(tfidf_text_val) #Get prediction probsblity scores\n",
    "print(type(prob))\n",
    "print(prob.size)\n",
    "print(type(prob_score))\n",
    "print(prob_score.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "7\n"
    }
   ],
   "source": [
    "test_prob_score = SVM_cl_p.predict_proba(tfidf_text_val[2])\n",
    "print(test_prob_score.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "This is prob: Machines en handgereedschap in index: 0\n     This is prob score: [0.12552724 0.12453582 0.14513011 0.12433426 0.12608752 0.28525015\n 0.0691349 ] at index: 0\n     This is prob score: [0.11851994 0.11596313 0.18978495 0.11519255 0.11681691 0.23350319\n 0.11021933] at index: 1\n     This is prob score: [0.13032124 0.1299462  0.14549248 0.1290827  0.13090292 0.09831778\n 0.23593667] at index: 2\n     This is prob score: [0.13122626 0.1333136  0.17180533 0.12997912 0.13186472 0.1740851\n 0.12772586] at index: 3\n     This is prob score: [0.12095807 0.12057714 0.16199772 0.11980851 0.12149796 0.09948358\n 0.25567703] at index: 4\n     This is prob score: [0.11716807 0.11679755 0.13364085 0.11605454 0.11769104 0.10775619\n 0.29089177] at index: 5\n     This is prob score: [0.11802951 0.11768984 0.26197413 0.11690779 0.11855632 0.12883363\n 0.13800878] at index: 6\nThis is prob: Machines en handgereedschap in index: 1\n     This is prob score: [0.12552724 0.12453582 0.14513011 0.12433426 0.12608752 0.28525015\n 0.0691349 ] at index: 0\n     This is prob score: [0.11851994 0.11596313 0.18978495 0.11519255 0.11681691 0.23350319\n 0.11021933] at index: 1\n     This is prob score: [0.13032124 0.1299462  0.14549248 0.1290827  0.13090292 0.09831778\n 0.23593667] at index: 2\n     This is prob score: [0.13122626 0.1333136  0.17180533 0.12997912 0.13186472 0.1740851\n 0.12772586] at index: 3\n     This is prob score: [0.12095807 0.12057714 0.16199772 0.11980851 0.12149796 0.09948358\n 0.25567703] at index: 4\n     This is prob score: [0.11716807 0.11679755 0.13364085 0.11605454 0.11769104 0.10775619\n 0.29089177] at index: 5\n     This is prob score: [0.11802951 0.11768984 0.26197413 0.11690779 0.11855632 0.12883363\n 0.13800878] at index: 6\nThis is prob: Vallen (van personen of voorwerpen) in index: 2\n     This is prob score: [0.12552724 0.12453582 0.14513011 0.12433426 0.12608752 0.28525015\n 0.0691349 ] at index: 0\n     This is prob score: [0.11851994 0.11596313 0.18978495 0.11519255 0.11681691 0.23350319\n 0.11021933] at index: 1\n     This is prob score: [0.13032124 0.1299462  0.14549248 0.1290827  0.13090292 0.09831778\n 0.23593667] at index: 2\n     This is prob score: [0.13122626 0.1333136  0.17180533 0.12997912 0.13186472 0.1740851\n 0.12772586] at index: 3\n     This is prob score: [0.12095807 0.12057714 0.16199772 0.11980851 0.12149796 0.09948358\n 0.25567703] at index: 4\n     This is prob score: [0.11716807 0.11679755 0.13364085 0.11605454 0.11769104 0.10775619\n 0.29089177] at index: 5\n     This is prob score: [0.11802951 0.11768984 0.26197413 0.11690779 0.11855632 0.12883363\n 0.13800878] at index: 6\nThis is prob: Machines en handgereedschap in index: 3\n     This is prob score: [0.12552724 0.12453582 0.14513011 0.12433426 0.12608752 0.28525015\n 0.0691349 ] at index: 0\n     This is prob score: [0.11851994 0.11596313 0.18978495 0.11519255 0.11681691 0.23350319\n 0.11021933] at index: 1\n     This is prob score: [0.13032124 0.1299462  0.14549248 0.1290827  0.13090292 0.09831778\n 0.23593667] at index: 2\n     This is prob score: [0.13122626 0.1333136  0.17180533 0.12997912 0.13186472 0.1740851\n 0.12772586] at index: 3\n     This is prob score: [0.12095807 0.12057714 0.16199772 0.11980851 0.12149796 0.09948358\n 0.25567703] at index: 4\n     This is prob score: [0.11716807 0.11679755 0.13364085 0.11605454 0.11769104 0.10775619\n 0.29089177] at index: 5\n     This is prob score: [0.11802951 0.11768984 0.26197413 0.11690779 0.11855632 0.12883363\n 0.13800878] at index: 6\nThis is prob: Vallen (van personen of voorwerpen) in index: 4\n     This is prob score: [0.12552724 0.12453582 0.14513011 0.12433426 0.12608752 0.28525015\n 0.0691349 ] at index: 0\n     This is prob score: [0.11851994 0.11596313 0.18978495 0.11519255 0.11681691 0.23350319\n 0.11021933] at index: 1\n     This is prob score: [0.13032124 0.1299462  0.14549248 0.1290827  0.13090292 0.09831778\n 0.23593667] at index: 2\n     This is prob score: [0.13122626 0.1333136  0.17180533 0.12997912 0.13186472 0.1740851\n 0.12772586] at index: 3\n     This is prob score: [0.12095807 0.12057714 0.16199772 0.11980851 0.12149796 0.09948358\n 0.25567703] at index: 4\n     This is prob score: [0.11716807 0.11679755 0.13364085 0.11605454 0.11769104 0.10775619\n 0.29089177] at index: 5\n     This is prob score: [0.11802951 0.11768984 0.26197413 0.11690779 0.11855632 0.12883363\n 0.13800878] at index: 6\nThis is prob: Vallen (van personen of voorwerpen) in index: 5\n     This is prob score: [0.12552724 0.12453582 0.14513011 0.12433426 0.12608752 0.28525015\n 0.0691349 ] at index: 0\n     This is prob score: [0.11851994 0.11596313 0.18978495 0.11519255 0.11681691 0.23350319\n 0.11021933] at index: 1\n     This is prob score: [0.13032124 0.1299462  0.14549248 0.1290827  0.13090292 0.09831778\n 0.23593667] at index: 2\n     This is prob score: [0.13122626 0.1333136  0.17180533 0.12997912 0.13186472 0.1740851\n 0.12772586] at index: 3\n     This is prob score: [0.12095807 0.12057714 0.16199772 0.11980851 0.12149796 0.09948358\n 0.25567703] at index: 4\n     This is prob score: [0.11716807 0.11679755 0.13364085 0.11605454 0.11769104 0.10775619\n 0.29089177] at index: 5\n     This is prob score: [0.11802951 0.11768984 0.26197413 0.11690779 0.11855632 0.12883363\n 0.13800878] at index: 6\nThis is prob: Andere vormen van contact met voorwerpen in index: 6\n     This is prob score: [0.12552724 0.12453582 0.14513011 0.12433426 0.12608752 0.28525015\n 0.0691349 ] at index: 0\n     This is prob score: [0.11851994 0.11596313 0.18978495 0.11519255 0.11681691 0.23350319\n 0.11021933] at index: 1\n     This is prob score: [0.13032124 0.1299462  0.14549248 0.1290827  0.13090292 0.09831778\n 0.23593667] at index: 2\n     This is prob score: [0.13122626 0.1333136  0.17180533 0.12997912 0.13186472 0.1740851\n 0.12772586] at index: 3\n     This is prob score: [0.12095807 0.12057714 0.16199772 0.11980851 0.12149796 0.09948358\n 0.25567703] at index: 4\n     This is prob score: [0.11716807 0.11679755 0.13364085 0.11605454 0.11769104 0.10775619\n 0.29089177] at index: 5\n     This is prob score: [0.11802951 0.11768984 0.26197413 0.11690779 0.11855632 0.12883363\n 0.13800878] at index: 6\nThis is prob: BHV inzet op locatie in index: 7\n     This is prob score: [0.12552724 0.12453582 0.14513011 0.12433426 0.12608752 0.28525015\n 0.0691349 ] at index: 0\n     This is prob score: [0.11851994 0.11596313 0.18978495 0.11519255 0.11681691 0.23350319\n 0.11021933] at index: 1\n     This is prob score: [0.13032124 0.1299462  0.14549248 0.1290827  0.13090292 0.09831778\n 0.23593667] at index: 2\n     This is prob score: [0.13122626 0.1333136  0.17180533 0.12997912 0.13186472 0.1740851\n 0.12772586] at index: 3\n     This is prob score: [0.12095807 0.12057714 0.16199772 0.11980851 0.12149796 0.09948358\n 0.25567703] at index: 4\n     This is prob score: [0.11716807 0.11679755 0.13364085 0.11605454 0.11769104 0.10775619\n 0.29089177] at index: 5\n     This is prob score: [0.11802951 0.11768984 0.26197413 0.11690779 0.11855632 0.12883363\n 0.13800878] at index: 6\nThis is prob: Vallen (van personen of voorwerpen) in index: 8\n     This is prob score: [0.12552724 0.12453582 0.14513011 0.12433426 0.12608752 0.28525015\n 0.0691349 ] at index: 0\n     This is prob score: [0.11851994 0.11596313 0.18978495 0.11519255 0.11681691 0.23350319\n 0.11021933] at index: 1\n     This is prob score: [0.13032124 0.1299462  0.14549248 0.1290827  0.13090292 0.09831778\n 0.23593667] at index: 2\n     This is prob score: [0.13122626 0.1333136  0.17180533 0.12997912 0.13186472 0.1740851\n 0.12772586] at index: 3\n     This is prob score: [0.12095807 0.12057714 0.16199772 0.11980851 0.12149796 0.09948358\n 0.25567703] at index: 4\n     This is prob score: [0.11716807 0.11679755 0.13364085 0.11605454 0.11769104 0.10775619\n 0.29089177] at index: 5\n     This is prob score: [0.11802951 0.11768984 0.26197413 0.11690779 0.11855632 0.12883363\n 0.13800878] at index: 6\nThis is prob: Vallen (van personen of voorwerpen) in index: 9\n     This is prob score: [0.12552724 0.12453582 0.14513011 0.12433426 0.12608752 0.28525015\n 0.0691349 ] at index: 0\n     This is prob score: [0.11851994 0.11596313 0.18978495 0.11519255 0.11681691 0.23350319\n 0.11021933] at index: 1\n     This is prob score: [0.13032124 0.1299462  0.14549248 0.1290827  0.13090292 0.09831778\n 0.23593667] at index: 2\n     This is prob score: [0.13122626 0.1333136  0.17180533 0.12997912 0.13186472 0.1740851\n 0.12772586] at index: 3\n     This is prob score: [0.12095807 0.12057714 0.16199772 0.11980851 0.12149796 0.09948358\n 0.25567703] at index: 4\n     This is prob score: [0.11716807 0.11679755 0.13364085 0.11605454 0.11769104 0.10775619\n 0.29089177] at index: 5\n     This is prob score: [0.11802951 0.11768984 0.26197413 0.11690779 0.11855632 0.12883363\n 0.13800878] at index: 6\nThis is prob: Vallen (van personen of voorwerpen) in index: 10\n     This is prob score: [0.12552724 0.12453582 0.14513011 0.12433426 0.12608752 0.28525015\n 0.0691349 ] at index: 0\n     This is prob score: [0.11851994 0.11596313 0.18978495 0.11519255 0.11681691 0.23350319\n 0.11021933] at index: 1\n     This is prob score: [0.13032124 0.1299462  0.14549248 0.1290827  0.13090292 0.09831778\n 0.23593667] at index: 2\n     This is prob score: [0.13122626 0.1333136  0.17180533 0.12997912 0.13186472 0.1740851\n 0.12772586] at index: 3\n     This is prob score: [0.12095807 0.12057714 0.16199772 0.11980851 0.12149796 0.09948358\n 0.25567703] at index: 4\n     This is prob score: [0.11716807 0.11679755 0.13364085 0.11605454 0.11769104 0.10775619\n 0.29089177] at index: 5\n     This is prob score: [0.11802951 0.11768984 0.26197413 0.11690779 0.11855632 0.12883363\n 0.13800878] at index: 6\n"
    }
   ],
   "source": [
    "for index in range(0, prob.size):\n",
    "    print(\"This is prob: {} in index: {}\".format(prob[index], index))\n",
    "    for i in range(0, 7):\n",
    "        print(\"     This is prob score: {} at index: {}\".format(prob_score[i],i))\n",
    "    if(index == 10):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Creating Predict File: 100.0%\nResults were saved in file:  result_test (2).csv\nPrediction Percentage:  0.6014799154334038\n"
    }
   ],
   "source": [
    "import time\n",
    "results = {\"text_1\":[],\"label_1\":[], \"probability_score\":[], \"correct\": []}\n",
    "pred_percentage=0\n",
    "pred_count=0\n",
    "score_count = 7\n",
    "for i in range (0,prob.size):\n",
    "    scores = []\n",
    "    results[\"text_1\"].append(df_ly[\"Incident_Description\"][i])\n",
    "    results[\"label_1\"].append(prob[i])\n",
    "    results[\"probability_score\"].append(prob_score[i])\n",
    "\n",
    "    if prob[i]==df_ly[\"Main_Categories\"][i]:\n",
    "        pred_count+=1\n",
    "        results[\"correct\"].append(True)\n",
    "    else:\n",
    "        results[\"correct\"].append(False)\n",
    "\n",
    "    p = ((i + 1) / prob.size) * 100\n",
    "    print (\"Creating Predict File: {}%\".format(round(p,2)), end=\"\\r\")\n",
    "\n",
    "result_filename = \"result_test (2).csv\"\n",
    "Results = pd.DataFrame(results)\n",
    "\n",
    "Results.to_csv(result_filename, index=False)\n",
    "print(\"\\nResults were saved in file: \", result_filename)\n",
    "pred_percentage= pred_count/prob.size\n",
    "print(\"Prediction Percentage: \", pred_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n                                 Aanrijding       0.00      0.00      0.00       154\nAgressie en extreme lichamelijke inspanning       1.00      0.07      0.13        29\n   Andere vormen van contact met voorwerpen       0.40      0.22      0.29       271\n                       BHV inzet op locatie       0.00      0.00      0.00        97\n    Gevaarlijke stoffen, brand en explosies       0.00      0.00      0.00        75\n                Machines en handgereedschap       0.52      0.34      0.41       193\n        Vallen (van personen of voorwerpen)       0.63      0.94      0.75      1073\n\n                                   accuracy                           0.60      1892\n                                  macro avg       0.36      0.22      0.23      1892\n                               weighted avg       0.48      0.60      0.51      1892\n\n<class 'str'>\n"
    }
   ],
   "source": [
    "report = classification_report(df_ly[\"Main_Categories\"], prob)\n",
    "print(report)\n",
    "print(type(report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real World Use Case example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import easygui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    print(\"\\n******************** Data Preprocessing ******************\\n\")\n",
    "\n",
    "    column_name = \"\"\n",
    "    column_exists = False\n",
    "\n",
    "    while(column_name == \"\" or column_exists == False):\n",
    "        column_name = input(\"Please indicate what column contains the Incident Reports: \")\n",
    "        print(\"Column Name: \", column_name)\n",
    "        for col in data.columns:\n",
    "            if(str(column_name) == str(col)):\n",
    "                column_exists = True\n",
    "                break\n",
    "\n",
    "        if(column_exists == False):\n",
    "            print(\"The column name you provided does not exist!\")\n",
    "\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "    stopword_list = set(nltk.corpus.stopwords.words('dutch'))\n",
    "\n",
    "    #add words that aren't in the NLTK stopwords list\n",
    "    added_stopwords = [\"b\", \"u\", \"e\", \"ste\", \"kv\", \"aa\", \"a9\", \"mtr\", '+', '1e', '2e', '<', '>', 'a', 'a1', 'a12', 'a2', 'a9',\"één\", \"gis\", \"app\", \"h\", 'o.a.', 'oa', 'en/of', \"uur\"\n",
    "    \"km\", \"mm\", \"v\", \"kg\", \"cm\", \"mm²\", \"mm2\", \"0ngever\", \"e\", \"de\", \"m\"]\n",
    "    updated_stopword_list = stopword_list.union(added_stopwords)\n",
    "\n",
    "    # Dutch spacy tokens \n",
    "    # nlp = nl_core_news_sm.load()\n",
    "\n",
    "    print(\"Loading Vectorizer...\")\n",
    "    \n",
    "\n",
    "    print(\"Data Preprocessing...\")\n",
    "    text = data[column_name].astype(str)\n",
    "    text = text.to_numpy()\n",
    "    # text_tfidf = tfidf_vectorizer.fit(text)\n",
    "    tfidf_text_val = train_tfidf.transform(text)\n",
    "\n",
    "    return tfidf_text_val, column_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, text, data, column):\n",
    "    print(\"\\n******************** Getting Predictions ******************\\n\")\n",
    "    prob = model.predict(text) #Get predictions\n",
    "    prob_score = model.predict_proba(text) #Get prediction probsblity scores\n",
    "    prob_results = {\"Incident_ID\": [],\"Incident_Report\":[],\"Model_Classification\":[], \"Probability_Scores\": []}\n",
    "    output_filename = \"\"\n",
    "\n",
    "    print(\"*Output will automatically be an Excel file (.xlsx)\")\n",
    "    \n",
    "    while(output_filename == \"\"):\n",
    "        output_filename = input(\"Please enter output file name: \")\n",
    "\n",
    "    output_filename = output_filename + \".xlsx\"\n",
    "    \n",
    "    for i in range (0,prob.size):\n",
    "        scores = []\n",
    "        prob_results[\"Incident_ID\"].append(data[\"Nr.\"][i])\n",
    "        prob_results[\"Incident_Report\"].append(data[column][i])\n",
    "        prob_results[\"Model_Classification\"].append(prob[i])\n",
    "        prob_results[\"Probability_Scores\"].append(prob_score[i])\n",
    "\n",
    "        p = ((i + 1) / prob.size) * 100\n",
    "        print (\"Creating Predict File: {}%\".format(round(p,2)), end=\"\\r\")\n",
    "\n",
    "    print(\"\\n\\nSaving Predict File...\")\n",
    "    Prob_Results = pd.DataFrame(prob_results)\n",
    "\n",
    "    print(\"\\n\\nCreating Excel File...\")\n",
    "    Prob_Results.to_excel(output_filename, index=False)\n",
    "    print(\"\\nResults were saved in file: \", output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "***************************************************************\nWelcome to the Indicent Report Classification System!\n***************************************************************\n\nPlease select file (.xlsx or .csv): \n\n******************** Data Preprocessing ******************\n\nColumn Name:  Beschrijving incident\nLoading Vectorizer...\nData Preprocessing...\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\micks\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n******************** Getting Predictions ******************\n\n*Output will automatically be an Excel file (.xlsx)\nCreating Predict File: 100.0%\n\nSaving Predict File...\n\n\nCreating Excel File...\n\nResults were saved in file:  OutPut.xlsx\n"
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"***************************************************************\")\n",
    "    print(\"Welcome to the Indicent Report Classification System!\")\n",
    "    print(\"***************************************************************\\n\")\n",
    "\n",
    "    try:\n",
    "        print(\"Please select file (.xlsx or .csv): \")\n",
    "        filename = easygui.fileopenbox()\n",
    "    except ValueError:\n",
    "        print(\"\\nPlease enter a valid file!\")\n",
    "        print(\"Please try again!\")\n",
    "    except:\n",
    "        print(\"Something went wrong, please ty again or conact... Somebody...\")\n",
    "\n",
    "    model_name = \"Category Ongeval(pototype).sav\"\n",
    "\n",
    "    # Dutch spacy tokens \n",
    "    nlp = nl_core_news_sm.load()\n",
    "\n",
    "    data = pd.read_excel(filename)\n",
    "\n",
    "    model = joblib.load(model_name)\n",
    "\n",
    "    text, column_name = preprocessing(data)\n",
    "\n",
    "    get_predictions(model, text, data, column_name)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Collecting openpyxl\n  Downloading openpyxl-2.6.4.tar.gz (173 kB)\nCollecting jdcal\n  Downloading jdcal-1.4.1-py2.py3-none-any.whl (9.5 kB)\nCollecting et_xmlfile\n  Downloading et_xmlfile-1.0.1.tar.gz (8.4 kB)\nUsing legacy setup.py install for openpyxl, since package 'wheel' is not installed.\nUsing legacy setup.py install for et-xmlfile, since package 'wheel' is not installed.\nInstalling collected packages: jdcal, et-xmlfile, openpyxl\n    Running setup.py install for et-xmlfile: started\n    Running setup.py install for et-xmlfile: finished with status 'done'\n    Running setup.py install for openpyxl: started\n    Running setup.py install for openpyxl: finished with status 'done'\nSuccessfully installed et-xmlfile-1.0.1 jdcal-1.4.1 openpyxl-2.6.4\n"
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bit7f6d5ef80ac84f0dbc4f0d76a489c0fa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}